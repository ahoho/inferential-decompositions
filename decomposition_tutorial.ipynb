{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome! This notebook describes a method of generating inferential decompositions from text as introduced in our EMNLP 2023 paper: [Natural Language Decompositions of Implicit Content Enable Better Text Representations](https://arxiv.org/pdf/2305.14583.pdf)!\n",
    "\n",
    "We will guide you through the process step-by-step, and provide explanations and code snippets along the way. The method can be broken down into the following steps:\n",
    "\n",
    "1. **Sample a small number of items from your dataset**: Here, we use a dataset of tweets posted by legislators during the 115, 116 and 117th US Congresses. \n",
    "2. **Craft Implicit and Explicit Propositions**: Refer to Appendix 2. of our paper for a description of the instructions used in the paper to craft exemplar poropotitions. We will use the same instructions to craft implicit and explicit propositions for our dataset.\n",
    "3. **Prompt an LLM with the crafted exemplars**: Here, we will use GPT3.5 Turbo for our experiments. \n",
    "4. **Validate**: Confirm that a random sample of the generated decompositions are _plausible_.\n",
    "5. **Downstream Usage**: Use the decompositions in the target task. \n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "To begin, run the first cell below to import the necessary packages and set up the environment. The helper functions and accompanying code are in `eval_mteb.py` and `generation_utils.py`. \n",
    "\n",
    "##### Note: \n",
    "We assume that your OPENAI_API_KEY is an environment variable. It can also be set manually in the config by setting  `config[\"llm\"][\"openai_api_key\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import GenerationConfig \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the data \n",
    "\n",
    "For the purpose of this tutorial, we choose a dataset of congressional tweets sampled from the 115th, 116th and 117th Congress. The data can be found in `data/sampled_tweets_senate_115-117.jsonl`.\n",
    "\n",
    "### Step 1: Sample a small number of items from your dataset\n",
    "\n",
    "We sampled the following tweets from the dataset to create our exemplar propositions.\n",
    "\n",
    "```\n",
    "1. \"The #HonestAds Act will strengthen protections against foreign interference in our election. No more election ads paid for in rubles.\"\n",
    "2. \"Our nation is hurting.\\\\n\\\\nGeorge Floyd's death was horrific and justice must be served. A single act of violence at the hands of an officer is one too many. \\\\n\\\\nGeorge Floyd deserved better. All black Americans do. Indeed, all Americans do.\"\n",
    "3. \"Happy Wyoming Day! Today, our great Equality State celebrates 151 years of being the first to officially recognize women's inherent right to vote and to hold office. \"\n",
    "4. \"\\\"More apologies from Mark Zuckerberg won't fix Facebook. We need accountability and action \\\\u2013 not vague commitments to do better while continuing to profit off of users' personal data. \"\n",
    "5. \"Finding a permanent solution to #ProtectDreamers is as urgent a task as ever. President Trump created this crisis, and he should stop tanking bipartisan congressional efforts to solve it. We owe it to these kids to keep them in the only country they've ever known as home.\"\n",
    "6. \"Qualified immunity reform should have as its focus professionalizing police departments, institutionalizing best police practices when it comes to use of force, and protecting constitutional rights of American citizens.\"\n",
    "7. \"Survivors of the coronavirus show symptoms of ME/CFS, a debilitating and chronic illness that already impacts 2.5 million Americans. I am fighting to secure the funding needed to treat this disease and give patients the care they need.\n",
    "```\n",
    "\n",
    "### Step 2: Craft Implicit and Explicit Propositions\n",
    "\n",
    "Then we craft both explicit and implicit propositions corresponding to each of the tweets, which can be found in `exemplars/leg_tweets_exemplars.jsonl`\n",
    "\n",
    "\n",
    "### Step 3: Prompting a LLM with the crafted exemplars\n",
    "\n",
    "We use the `GenerationEmbedder` class from `eval_mteb.py` along with the hyperparameters specified in `configs/leg-tweet-gen-gpt3.5-propositions-all.yaml` to prompt GPT3.5 Turbo with the exemplars. The generated decompositions can be found in `data/gpt3.5_tweets_to_gen_all.jsonl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/clip-political/rupak/miniconda3/envs/hf/lib/python3.10/site-packages/langchain/llms/openai.py:716: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from eval_mteb import  GenerationEmbedder, read_jsonl, load_config, write_jsonl\n",
    "\n",
    "TWEETS_FILEPATH = Path('data/sampled_tweets_senate_115-117.jsonl')\n",
    "tweets = read_jsonl(TWEETS_FILEPATH)\n",
    "\n",
    "# load the config file and the exemplars \n",
    "config = load_config('configs/leg-tweet-gen-gpt3.5-propositions-all.yaml')\n",
    "exemplars = read_jsonl(config[\"data\"]['exemplars_path'])\n",
    "\n",
    "# initialize the generation object with hyperparameters loaded from the config file\n",
    "model = GenerationEmbedder(\n",
    "    instructions=config[\"data\"][\"instructions\"],\n",
    "    openai_api_key=config[\"llm\"][\"openai_api_key\"],\n",
    "    exemplar_pool=exemplars,\n",
    "    exemplar_format=config[\"exemplars\"][\"format\"],\n",
    "    exemplar_sep=config[\"exemplars\"][\"separator\"],\n",
    "    multi_output_sep=config[\"exemplars\"][\"multi_output_separator\"],\n",
    "    exemplars_per_prompt=config[\"exemplars\"][\"exemplars_per_prompt\"],\n",
    "    draws_per_pool=config[\"exemplars\"][\"draws_per_pool\"],\n",
    "    repeat_draws=config[\"exemplars\"][\"repeat_draws\"],\n",
    "    shuffles_per_draw=config[\"exemplars\"][\"shuffles_per_draw\"],\n",
    "    output_combination_strategy=config[\"embeddings\"][\"output_combination_strategy\"],\n",
    "    include_original_doc=config[\"embeddings\"][\"include_original_doc\"],\n",
    "    embedding_model_name=config[\"embeddings\"][\"embedding_model_name\"],\n",
    "    gen_model_name=config[\"llm\"][\"gen_model_name\"],\n",
    "    generations_per_prompt=config[\"llm\"][\"generations_per_prompt\"],\n",
    "    temperature=config[\"llm\"][\"temperature\"],\n",
    "    top_p=config[\"llm\"][\"top_p\"],\n",
    "    generation_kwargs=config[\"llm\"][\"generation_kwargs\"],\n",
    "    max_tokens=config[\"llm\"][\"max_tokens\"],\n",
    "    cache_db_path=config[\"main\"][\"cache_db_path\"],\n",
    "    dry_run=config[\"main\"][\"dry_run\"],\n",
    "    device=config[\"embeddings\"][\"device\"],\n",
    "    seed=config[\"main\"][\"seed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of the tutorial, we are generating decompositions for the first 10 tweets, decompositions for the whole dataset can be found in `data/gpt3.5_tweets_to_gen_all.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate propositions from tweets \n",
    "# simple batching code that deals with breaks in connections\n",
    "\n",
    "# use a small sample of tweets to test the generations\n",
    "OUTPUT_PATH = Path(\"test.jsonl\")\n",
    "\n",
    "tweet_texts = [tweet['tweet'] for tweet in tweets][:10] # remove [:10] to run on all tweets\n",
    "\n",
    "propositions = read_jsonl(OUTPUT_PATH)\n",
    "batch_size = 100\n",
    "for index in tqdm(range(len(propositions), len(tweet_texts), batch_size)):\n",
    "    batch = tweet_texts[index:index+batch_size]\n",
    "    propositions.extend(model.generate_from_inputs(batch))\n",
    "    write_jsonl(propositions, OUTPUT_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Validate\n",
    "\n",
    "We sample some of the generated decompositions and confirm that they are _plausible_. In our paper, this was done using a human study. Please refer to Section 3 of our paper for more details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEET: Cindy &amp; I are praying for all those in the path of #HurricaneIrma. We thank the brave volunteers &amp; urge all to listen to local officials.\n",
      "PROPOSITONS:\n",
      "Cindy and I are offering prayers for those affected by Hurricane Irma\n",
      "Gratitude towards brave volunteers\n",
      "People should follow the guidance of local officials during the hurricane\n",
      "Hurricane Irma poses a significant threat\n",
      "---------------------------\n",
      "TWEET: We must do more to address mental health issues our veterans face and ensure all have access to treatment. @WSAZnews #suicidepreventionmonth  \n",
      "PROPOSITONS:\n",
      "Veterans face mental health issues that need to be addressed\n",
      "Access to mental health treatment for veterans should be ensured\n",
      "Suicide prevention is important\n",
      "Mental health support for veterans needs improvement\n",
      "---------------------------\n",
      "TWEET: This project will bring hundreds of jobs &amp; millions of $ in economic growth to Northwest MT. #EmployMT #ConnectMT\n",
      " \n",
      "PROPOSITONS:\n",
      "The project will create job opportunities in Northwest Montana\n",
      "Economic growth will result from the project\n",
      "Montana will benefit from job creation and increased revenue\n",
      "Investing in Montana's economy is important\n",
      "Job creation is essential for economic development\n",
      "---------------------------\n",
      "TWEET: The STOP School Violence Act provides grants for evidence-based security infrastructure improvements in our schools. \n",
      "\n",
      " \n",
      "PROPOSITONS:\n",
      "The STOP School Violence Act provides grants for school security improvements\n",
      "School safety is a priority\n",
      "Evidence-based measures should be implemented in schools\n",
      "Grants should be provided to enhance school security infrastructure\n",
      "---------------------------\n",
      "TWEET: One of the most destructive aspects of “Anger Politics” is it uses the other sides outrageous rhetoric to justify or downplay the same behavior from your side. Just tonight someone went on national tv &amp; described  opposition to  “caravan” as a modern day Reichstag fire. Poison.\n",
      "PROPOSITONS:\n",
      "\"Anger Politics\" is destructive\n",
      "Using outrageous rhetoric to justify or downplay behavior is problematic\n",
      "Comparison of opposition to the \"caravan\" to the Reichstag fire is unfair\n",
      "Comparisons to historical events can be misleading\n",
      "Extreme rhetoric poisons political discourse.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# before sampling, make sure to keep the tweet with the generations: \n",
    "\n",
    "for tweet_text, props in zip(tweet_texts, propositions):\n",
    "    props.append(tweet_text)\n",
    "\n",
    "# sample from the propositions\n",
    "random.seed(42)\n",
    "sample = random.sample(propositions, 5)\n",
    "\n",
    "for elem in sample: \n",
    "    print(f\"TWEET: {elem[-1]}\")\n",
    "    print(\"PROPOSITONS:\")\n",
    "    for prop in elem[:-1]:\n",
    "        print(prop)\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Use the propositions for your own downstream task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
